{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fe2d23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e175a30c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2402cc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Time_Series_GPT(nn.Module):\n",
    "    \n",
    "    def __init__(self, params_obj):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.output_size  = params_obj.output_size\n",
    "        self.seq_length   = params_obj.seq_length       ## 40 or 15\n",
    "        self.input_size   = params_obj.input_size\n",
    "        self.num_features = params_obj.num_features\n",
    "        self.device       = params_obj.device\n",
    "        self.block_size   = params_obj.block_size\n",
    "        self.MyName       = \"The GPT model\"\n",
    "\n",
    "        self.pos_emb_table   = nn.Embedding(params_obj.block_size, params_obj.n_embd)      ## [block, 512] or [40, 512]\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "                *[  Block(params_obj.n_embd, params_obj.n_head, params_obj.block_size) for _ in range(params_obj.n_layer)  ]\n",
    "        )\n",
    "        \n",
    "        self.ln_f        = nn.LayerNorm(  params_obj.n_embd    )        \n",
    "        self.lm_ffw_head = nn.Linear(params_obj.n_embd, self.num_features)      ## [512, 65] # FFW Layer\n",
    "\n",
    "        #######################################################################\n",
    "\n",
    "        self.map_24_512 = nn.Linear(self.num_features, 512)  ## [24, 512] # projection\n",
    "        self.map_act    = nn.ReLU()\n",
    "\n",
    "        #######################################################################\n",
    "\n",
    "        self.map_24_512_1 = nn.Linear(self.num_features, 100)  ## [24, 512] # projection\n",
    "        self.map_act1     = nn.ReLU()\n",
    "        self.LayerNorm1   = nn.LayerNorm( 100 )\n",
    "        \n",
    "        self.map_24_512_2 = nn.Linear(100, 200)      ## [24, 512] # projection\n",
    "        self.map_act2     = nn.ReLU()\n",
    "        self.LayerNorm2   = nn.LayerNorm( 200 )\n",
    "        \n",
    "        self.map_24_512_3 = nn.Linear(200, 512)      ## [24, 512] # projection\n",
    "        \n",
    "        self.dropout_24_512 = nn.Dropout(0.2)\n",
    "   \n",
    "\n",
    "    def forward(self,  idx, targets):\n",
    "\n",
    "        B = idx.shape[0]       ## 16 batch \n",
    "        T = idx.shape[1]       ## 40 or 15\n",
    "\n",
    "        ############################################################\n",
    "\n",
    "        idx = self.map_24_512_1( idx )       ## (8, 15, 24) goes in\n",
    "        idx = self.map_act1(     idx )\n",
    "        idx = self.dropout_24_512(     idx )\n",
    "        idx = self.LayerNorm1(   idx )\n",
    "        \n",
    "        idx = self.map_24_512_2( idx )\n",
    "        idx = self.map_act2(     idx )\n",
    "        idx = self.dropout_24_512(     idx )\n",
    "        idx = self.LayerNorm2(   idx )\n",
    "        \n",
    "        idx = self.map_24_512_3( idx )\n",
    "        \n",
    "        tok_emb = idx                        ## (B, 15, 512)\n",
    "\n",
    "        ###########################################################\n",
    "        \n",
    "        pos_emb = self.pos_emb_table( torch.arange(T, device=self.device) )  \n",
    "        \n",
    "        ###########################################################\n",
    "        \n",
    "        ## x = tok_emb + pos_emb + conv_emb + per_conv_emb    ## [B, T, E] or [N, 40, 512], now [N, 15, 24]\n",
    "\n",
    "        x = tok_emb + pos_emb \n",
    "\n",
    "        ############################################################\n",
    "        \n",
    "        x = self.blocks(  x  )               ## (B, T, E)   \n",
    "        x = self.ln_f(    x  )               ## (B, T, E)   ## norm\n",
    "        logits = self.lm_ffw_head(x)         ## [B, 40, 65]  or [N, 15, 24]\n",
    "        return logits\n",
    "        \n",
    "        \n",
    "    def generate(self, idx, max_new_tokens):    ## idx is (B, T)\n",
    "        print(\"max tokens \", max_new_tokens)\n",
    "        print(idx.shape)\n",
    "        for _ in range(max_new_tokens):\n",
    "            ## crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size:, :]\n",
    "            logits = self(idx_cond, 0 )    ## ## get preds\n",
    "            logits = logits[:, -1, :]    ## focus on last one (B, E)\n",
    "            logits = logits.unsqueeze(0)\n",
    "            ## probs = F.softmax(logits, dim= -1)    ## (B, E) get probs\n",
    "            ## idx_next = torch.multinomial(probs, num_samples=1)     ## (B, 1) selected\n",
    "            idx = torch.cat(  (idx, logits), dim=1  )   ## (B, T+1) append sample to running sequence\n",
    "            \n",
    "        return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91598740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9156e93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67432e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward(self, idx, targets=None, reasoning_steps=1):\n",
    "    \"\"\"\n",
    "    Forward method supporting both standard and iterative (reasoning) modes.\n",
    "    \n",
    "    Args:\n",
    "        idx (Tensor): Input tensor of shape [B, T, F]\n",
    "        targets (Tensor or None): Optional targets for loss calculation\n",
    "        reasoning_steps (int): Number of reasoning iterations. If 1, use normal mode.\n",
    "\n",
    "    Returns:\n",
    "        logits: Final prediction (B, F)\n",
    "        loss (optional): If targets provided, return MSE loss\n",
    "    \"\"\"\n",
    "    B, T, _ = idx.shape\n",
    "\n",
    "    # Clone to avoid modifying original input during reasoning\n",
    "    working_idx = idx.clone()\n",
    "\n",
    "    for step in range(reasoning_steps):\n",
    "        # Crop for causal context if needed\n",
    "        idx_cond = working_idx[:, -self.block_size:, :]\n",
    "\n",
    "        # === Begin Core Forward ===\n",
    "        # Projection MLP\n",
    "        x = self.map_24_512_1(idx_cond)\n",
    "        x = self.map_act1(x)\n",
    "        x = self.dropout_24_512(x)\n",
    "        x = self.LayerNorm1(x)\n",
    "\n",
    "        x = self.map_24_512_2(x)\n",
    "        x = self.map_act2(x)\n",
    "        x = self.dropout_24_512(x)\n",
    "        x = self.LayerNorm2(x)\n",
    "\n",
    "        x = self.map_24_512_3(x)\n",
    "\n",
    "        # Add positional encoding\n",
    "        pos_emb = self.pos_emb_table(torch.arange(x.shape[1], device=self.device))\n",
    "        x = x + pos_emb\n",
    "\n",
    "        # Transformer blocks\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_ffw_head(x)\n",
    "        # === End Core Forward ===\n",
    "\n",
    "        # Extract latest stepâ€™s prediction\n",
    "        last_pred = logits[:, -1:, :]  # shape: (B, 1, F)\n",
    "\n",
    "        # In reasoning mode, feed it back in\n",
    "        if reasoning_steps > 1 and step < reasoning_steps - 1:\n",
    "            # Detach to stop gradient unless we want full backprop (optional)\n",
    "            working_idx = torch.cat((working_idx, last_pred.detach()), dim=1)\n",
    "\n",
    "    final_pred = last_pred.squeeze(1)  # shape: (B, F)\n",
    "\n",
    "    if targets is not None:\n",
    "        loss = F.mse_loss(final_pred, targets)\n",
    "        return final_pred, loss\n",
    "\n",
    "    return final_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d545e35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830bd2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b4fa7a6",
   "metadata": {},
   "source": [
    "\n",
    "## Merge these\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53916581",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Time_Series_GPT(nn.Module):\n",
    "    \n",
    "    def __init__(self, params_obj):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.output_size  = params_obj.output_size\n",
    "        self.seq_length   = params_obj.seq_length       ## 40 or 15\n",
    "        self.input_size   = params_obj.input_size\n",
    "        self.num_features = params_obj.num_features\n",
    "        self.device       = params_obj.device\n",
    "        self.block_size   = params_obj.block_size\n",
    "        self.MyName       = \"The GPT model\"\n",
    "\n",
    "        self.pos_emb_table   = nn.Embedding(params_obj.block_size, params_obj.n_embd)      ## [block, 512] or [40, 512]\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "                *[  Block(params_obj.n_embd, params_obj.n_head, params_obj.block_size) for _ in range(params_obj.n_layer)  ]\n",
    "        )\n",
    "        \n",
    "        self.ln_f        = nn.LayerNorm(  params_obj.n_embd    )        \n",
    "        self.lm_ffw_head = nn.Linear(params_obj.n_embd, self.num_features)      ## [512, 65] # FFW Layer\n",
    "\n",
    "        #######################################################################\n",
    "\n",
    "        self.map_24_512 = nn.Linear(self.num_features, 512)  ## [24, 512] # projection\n",
    "        self.map_act    = nn.ReLU()\n",
    "\n",
    "        #######################################################################\n",
    "\n",
    "        self.map_24_512_1 = nn.Linear(self.num_features, 100)  ## [24, 512] # projection\n",
    "        self.map_act1     = nn.ReLU()\n",
    "        self.LayerNorm1   = nn.LayerNorm( 100 )\n",
    "        \n",
    "        self.map_24_512_2 = nn.Linear(100, 200)      ## [24, 512] # projection\n",
    "        self.map_act2     = nn.ReLU()\n",
    "        self.LayerNorm2   = nn.LayerNorm( 200 )\n",
    "        \n",
    "        self.map_24_512_3 = nn.Linear(200, 512)      ## [24, 512] # projection\n",
    "        \n",
    "        self.dropout_24_512 = nn.Dropout(0.2)\n",
    "   \n",
    "\n",
    "    def forward(self,  idx, targets=None, reasoning_steps=1, backprop_through_steps=False):\n",
    "        \n",
    "        ## targets (Tensor or None): Optional targets for loss calculation\n",
    "        ## reasoning_steps (int): Number of reasoning iterations. If 1, use normal mode.\n",
    "        ## logits: Final prediction (B, F)\n",
    "        ## loss (optional): If targets provided, return MSE loss\n",
    "        \n",
    "        B = idx.shape[0]       ## 16 batch \n",
    "        T = idx.shape[1]       ## 40 or 15\n",
    "\n",
    "        # Clone to avoid modifying original input during reasoning\n",
    "        working_idx = idx.clone()\n",
    "        \n",
    "        ############################################################\n",
    "        \n",
    "        for step in range(reasoning_steps):\n",
    "            \n",
    "            # Crop for causal context if needed, from 1..21, removes 1, so that 1..20\n",
    "            idx_cond = working_idx[:, -self.block_size:, :]     ## makes sure just block size, \n",
    "            \n",
    "            # === Begin Core Forward ===\n",
    "            # Projection MLP\n",
    "            \n",
    "            x = self.map_24_512_1(idx_cond)   ## (8, 15, 24) goes in\n",
    "            x = self.map_act1(x)\n",
    "            x = self.dropout_24_512(x)\n",
    "            x = self.LayerNorm1(x)\n",
    "            \n",
    "            x = self.map_24_512_2(x)\n",
    "            x = self.map_act2(x)\n",
    "            x = self.dropout_24_512(x)\n",
    "            x = self.LayerNorm2(x)\n",
    "            \n",
    "            x = self.map_24_512_3(x)\n",
    "            \n",
    "            pos_emb = self.pos_emb_table(torch.arange(T, device=self.device) )\n",
    "            \n",
    "            ## x = tok_emb + pos_emb + conv_emb + per_conv_emb  ## [B, T, E] or [N, 40, 512], now [N, 15, 24]\n",
    "            ## x = tok_emb + pos_emb \n",
    "            \n",
    "            x = x + pos_emb       ## (B, 15, 512)\n",
    "                       \n",
    "            # Transformer blocks\n",
    "            x = self.blocks(x)      ## (B, T, E)\n",
    "            x = self.ln_f(x)        ## (B, T, E)   ## normalization\n",
    "            logits = self.lm_ffw_head(x)     ## [B, 40, 65]  or [N, 15, 24] \n",
    "            # === End Core Forward ===\n",
    "            \n",
    "            # Extract latest stepâ€™s prediction\n",
    "            last_pred = logits[:, -1:, :]  # shape: (B, 1, F)\n",
    "            \n",
    "\n",
    "            # In reasoning mode, feed it back in\n",
    "            if reasoning_steps > 1 and step < reasoning_steps - 1:\n",
    "                \n",
    "                if backprop_through_steps:\n",
    "                    working_idx = torch.cat((working_idx, last_pred), dim=1)\n",
    "                else:\n",
    "                    ## Detach to stop gradient unless we want full backprop (optional)\n",
    "                    working_idx = torch.cat((working_idx, last_pred.detach()), dim=1)\n",
    "            \n",
    "\n",
    "        ############################################################\n",
    "       \n",
    "        final_pred = last_pred.squeeze(1)  # shape: (B, F)\n",
    "\n",
    "        return final_pred\n",
    "\n",
    "\n",
    "            \n",
    "    def generate(self, idx, max_new_tokens):    ## idx is (B, T)\n",
    "        print(\"max tokens \", max_new_tokens)\n",
    "        print(idx.shape)\n",
    "        for _ in range(max_new_tokens):\n",
    "            ## crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size:, :]\n",
    "            logits = self(idx_cond, 0 )    ## ## get preds\n",
    "            logits = logits[:, -1, :]    ## focus on last one (B, E)\n",
    "            logits = logits.unsqueeze(0)\n",
    "            ## probs = F.softmax(logits, dim= -1)    ## (B, E) get probs\n",
    "            ## idx_next = torch.multinomial(probs, num_samples=1)     ## (B, 1) selected\n",
    "            idx = torch.cat(  (idx, logits), dim=1  )   ## (B, T+1) append sample to running sequence\n",
    "            \n",
    "        return idx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5946bede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162dccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Without backprop through reasoning chain\n",
    "output = model(idx, reasoning_steps=10, backprop_through_steps=False)\n",
    "loss = F.mse_loss(output, targets)\n",
    "\n",
    "# With full backprop (heavier)\n",
    "output = model(idx, reasoning_steps=10, backprop_through_steps=True)\n",
    "loss = F.mse_loss(output, targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e2d279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbc4f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157cc147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "536099d2",
   "metadata": {},
   "source": [
    "\n",
    "## the loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b5ccdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a7838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def function_train_rc(train_CIVS_2, reasoning_steps=1, backprop_through_steps=False):\n",
    "    \n",
    "    \n",
    "    # === Model Setup ===\n",
    "    model = Time_Series_GPT.Time_Series_GPT(tsGPT_obj).to(tsGPT_obj.device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=tsGPT_obj.learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "    # === History ===\n",
    "    history = {\n",
    "        'loss': [], 'test_loss': [],\n",
    "        'loss_A': [], 'loss_B': [], 'loss_C': [],\n",
    "        'loss_SIM0': [], 'loss_SIM1': [], 'loss_SIM2': [],\n",
    "        'loss_SIM3': [], 'loss_SIM4': [], 'loss_SIM5': []\n",
    "    }\n",
    "\n",
    "    # === Data Scaling ===\n",
    "    train_CIVS_tr, x_means, x_standard_devs = tsGPT_obj.standardize_x_scales(train_CIVS_2)\n",
    "    train_CIVS_tr_scaled = (train_CIVS_tr - x_means) / x_standard_devs\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "\n",
    "    for batch_i, epoch in enumerate(range(tsGPT_obj.max_iters)):\n",
    "        xb, yb = tsGPT_obj.get_batch(train_CIVS_tr_scaled)  # shapes: [B, T, F]\n",
    "        \n",
    "   \n",
    "    logits   = model( xb, yb )\n",
    "        B, T, E  = logits.shape\n",
    "        \n",
    "        # === Forward Pass with Reasoning Mode ===\n",
    "        pred = model(\n",
    "            xb, \n",
    "            targets=None, \n",
    "            reasoning_steps=reasoning_steps,\n",
    "            backprop_through_steps=backprop_through_steps\n",
    "        )  # pred: [B, F] (only the last timestep)\n",
    "        \n",
    "        \n",
    "        #####################################################################################\n",
    "        \n",
    " \n",
    "        # === Match dimensions for loss ===\n",
    "        B, T, E = xb.shape\n",
    "        y_train = yb[:, -1, :]  # last timestep only\n",
    "        pred = pred.view(B, E)\n",
    "\n",
    "        # === Loss based on deltas (per your original) ===\n",
    "        # You used deltas across timesteps; here it's a bit tricky since pred is just last step\n",
    "        # Instead, we'll simulate your original by comparing to last yb timestep\n",
    "        loss_SI  = criterion(pred[:, 2], y_train[:, 2])       # special loss on feature 2\n",
    "        loss_all = criterion(pred, y_train)                   # full loss\n",
    "        \n",
    "        \n",
    "        \n",
    "        #####################################################################################\n",
    "\n",
    "        loss_total = loss_SI\n",
    "\n",
    "        # === Backprop ===\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "         \n",
    "\n",
    "        # === Logging ===\n",
    "        history['loss'].append(loss_total.item())\n",
    "        history['loss_A'].append(loss_SI.item())\n",
    "        history['loss_B'].append(loss_all.item())\n",
    "        history['loss_C'].append(0)\n",
    "\n",
    "        # Placeholders if Curve_SIMs aren't defined\n",
    "        history['loss_SIM0'].append(0)\n",
    "        history['loss_SIM1'].append(0)\n",
    "        \n",
    "\n",
    "        # === Debug Print ===\n",
    "        if batch_i % 500 == 0:\n",
    "            print(f\"[{batch_i}] Train Loss: {loss_total.item():.6f}\")\n",
    "            print(\"Loss_SI:\", loss_SI.item(), \"Loss_all:\", loss_all.item())\n",
    "            print(\"Prediction shape:\", pred.shape)\n",
    "            print(\"===\")\n",
    "\n",
    "    tsGPT_obj.plot_losses_training(history)\n",
    "    return model, history, x_means, x_standard_devs, train_CIVS_tr_scaled\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0f9994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a598764c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb21368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e775de53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873cb12b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef623fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model, history, x_means, x_std, scaled_data = function_train_rc(\n",
    "    train_CIVS_2,\n",
    "    reasoning_steps=10,\n",
    "    backprop_through_steps=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7866d809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2b6d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4186abc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3582b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b0ad18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aa744e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d542f594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da407be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d847d0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14970c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
